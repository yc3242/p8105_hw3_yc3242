---
title: "p8105_hw3_yc3242"
author: "Youn Kyeong Chang (uni# yc3242)"
date: "October 10, 2018"
output: github_document
---

I used the tidyverse library and some options for outset through the whole problem set.

```{r}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```


# Problem 1

First, I loaded the dataset for problem 1 from the following library.

```{r brfss_import}
library(p8105.datasets)
data("brfss_smart2010")
```

Next, I cleaned the data and focused on the `Overall Health` topic and organized responses as factor taking levels ordered from "Excellent" to "Poor".

```{r brfss_manipulation}
response_levels = c("Excellent", "Very good", "Good", "Fair", "Poor")

brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>%                          # clean variable names
  rename(state = locationabbr,
         location = locationdesc) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = response_levels))
```

First, to find out the states which were observed at 7 locations in 2002, I retained distinct rows by state and location and counted the number of states. 

```{r n_locations}
brfss_smart2010 %>% 
  filter(year == 2002) %>%
  distinct(state, location) %>% 
  count(state)  %>% 
  filter(n == 7)
```

In 2002, **CT, FL, NC** were observed at 7 locations.


Next, I made a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r}
brfss_smart2010 %>% 
  group_by(state, year) %>%
  distinct(location) %>%
  mutate(n_location = n()) %>% 
  ggplot(aes(x = year, y = n_location)) +
  geom_line(aes(color = state)) +
  labs(
    title = "Spaghetti plot of the number of locations across states from 2002 to 2010",
    x = "Year",
    y = "Number of locations"
  )
  viridis::scale_color_viridis(
    name = "State",
    discrete = TRUE
    ) 
```

This spaghetti plot shows that the number of locations of state seems constant, fluctuating below 20 except that there is a huge surge in 2007 and 2010 in Florida.  

Then, I made a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r}
brfss_smart2010 %>%
  filter(year %in% c(2002, 2006, 2010), 
         state == "NY", 
         response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean = mean(data_value), sd = sd(data_value)) %>% 
  knitr::kable(digits = 2)
```

The average and deviation of the proportion of Excellent responses across locations in NY state were the highest in 2002. 


For each year and state, I computed the average proportion in each response category (taking the average across locations in a state) and made a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
brfss_smart2010 %>% 
  group_by(year, state, response) %>% 
  summarize(mean = mean(data_value)) %>% 
  ggplot(aes(x = mean, fill = as.factor(year))) +    
  geom_density(alpha = .4) +
  facet_grid(~ response) +
  labs(
    x = "Average",
    y = " "
  ) +
  scale_x_continuous(breaks = c(0, 10, 20, 30, 40, 50),
                     limits = c(0, 45)) +
  scale_y_continuous(limits = c(0, 0.5)) +
  viridis::scale_fill_viridis(
    name = "Year",
    discrete = TRUE) 
```


The order of average proportion is Very good > Good > Excellent > Fair > Poor from the highest and the lowest. Even though the poor response has the lowest average proportion, the density is the highest and its deviation is very narrow. Given that for every category, the density plots by years are overlapped, we can conclude that the response is similar over time.   


# Problem 2

First, I loaded the dataset for problem 2 from the following library.

```{r instacart_import}
library(p8105.datasets)
data("instacart") %>% 
  janitor::clean_names()
```

There are **`r instacart %>% distinct(aisle) %>% count()`** aisles in the dataset.
The aisles below are the most items ordered from. 

```{r}
instacart %>% 
  count(aisle) %>% 
  top_n(3)
```

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r}
instacart %>%
  count(aisle) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n, .desc = TRUE)) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_bar(stat = "identity") +
  coord_flip()
```

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  filter(min_rank(desc(n)) == 1)
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
days = 
  c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")

instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean = mean(order_hour_of_day)) %>% 
  mutate(order_dow = factor(days[order_dow + 1], levels = days)) %>% 
  spread(key = order_dow, value = mean)
```

To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. Then, do or answer the following (commenting on the results of each):