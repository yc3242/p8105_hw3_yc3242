---
title: "p8105_hw3_yc3242"
author: "Youn Kyeong Chang (uni# yc3242)"
date: "October 10, 2018"
output: github_document
---

I used the tidyverse library through the whole problem set.

```{r}
library(tidyverse)
```


# Problem 1

First, I loaded the dataset for problem 1 from the following library.

```{r brfss_import}
library(p8105.datasets)
data("brfss_smart2010")
```

Next, I cleaned the data and focused on the `Overall Health` topic and organized responses as factor taking levels ordered from "Excellent" to "Poor".

```{r brfss_manipulation}
response_levels = c("Excellent", "Very good", "Good", "Fair", "Poor")

brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>%                         
  rename(state = locationabbr,
         location = locationdesc) %>% 
  filter(topic == "Overall Health") %>%           
  mutate(response = factor(response, levels = response_levels))
```

To find out the states which were observed at 7 locations in 2002, I retained distinct rows by state and location and counted the number of states. 

```{r n_locations_2002}
brfss_smart2010 %>% 
  filter(year == 2002) %>%
  distinct(state, location) %>% 
  count(state)  %>% 
  filter(n == 7)
```

In 2002, **CT, FL, NC** were observed at 7 locations.

Then, I made a spaghetti plot that shows the number of locations in each state from 2002 to 2010.

```{r location_2002_2010, fig.width = 6, fig.asp = .6, out.width = "90%"}
brfss_smart2010 %>% 
  group_by(state, year) %>%
  distinct(location) %>%
  mutate(n_location = n()) %>% 
  ggplot(aes(x = year, y = n_location, color = state)) +
   geom_line(alpha = 0.4) +
   labs(
     title = "Number of locations across states from 2002 to 2010",
     x = "Year",
     y = "Number of locations"
     ) +
  viridis::scale_color_viridis(
    name = "State",
    discrete = TRUE
    ) +
  theme(plot.title = element_text(size = 12),
        legend.position = "bottom",
        legend.key.width = unit(.3, "cm"),
        legend.text = element_text(size = 7)) +
  guides(color = guide_legend(ncol = 17)) 
```

This spaghetti plot shows that from 2002 to 2010, the number of locations of state seems constant, slightly fluctuating below 20. However, this plot is not readable since many lines are overlapped. Thus, I re-plotted with a subset of some states whose maximum number of location is greater than 15.

```{r location_2002_2010_subset, fig.width = 6, fig.asp = .6, out.width = "90%"}
brfss_smart2010 %>% 
  group_by(state, year) %>%
  distinct(location) %>%
  summarize(n_location = n()) %>% 
  group_by(state) %>% 
  mutate(max_n_location = max(n_location)) %>% 
  filter(max_n_location > 15) %>% 
  ggplot(aes(x = year, y = n_location, color = state)) +
   geom_line(alpha = 0.7) +
   labs(
     title = "Number of locations across states from 2002 to 2010",
     x = "Year",
     y = "Number of locations",
     caption = "subset of data above"
     ) +
  viridis::scale_color_viridis(
    name = "State",
    discrete = TRUE
    ) +
  theme(plot.title = element_text(size = 12),
        legend.position = "bottom",
        legend.key.width = unit(.5, "cm"),
        legend.text = element_text(size = 9)) 
```

From this plot, I can not only see that there was a surge in number of locations in FL in 2007 and 2010 but NC, NJ and TX have generally more locations than other states. 

Also, I made a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r ex_table}
brfss_smart2010 %>%
  filter(year %in% c(2002, 2006, 2010), 
         state == "NY", 
         response == "Excellent") %>% 
  group_by(year) %>% 
  summarize("Average proportion of Excellent(%) " = mean(data_value), 
            "Standard deviation of proportion of Excellent(%)" = sd(data_value)) %>% 
  knitr::kable(digits = 2)
```

The average and deviation of the proportion of Excellent responses across locations in NY state were the highest in 2002 and the values are similar in 2006 and 2010. 

For each year and state, I computed the average proportion in each response category (taking the average across locations in a state) and made a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r response_five_panel, fig.width = 6, fig.asp = .6, out.width = "90%"}
brfss_smart2010 %>% 
  group_by(year, state, response) %>% 
  summarize(mean_response = mean(data_value)) %>% 
  ggplot(aes(x = factor(year), y = mean_response)) +     
    geom_violin(aes(fill = factor(year))) +
    stat_summary(fun.y = mean, geom = "point", color = "black") +
    facet_grid( ~ response) +
    labs(
      x = "Year",
      y = "Average proportion(%)",
      title = "Average proportion in each response category from 2002 to 2010"
    ) +
  viridis::scale_fill_viridis(
    name = "State",
    discrete = TRUE) +
  theme(plot.title = element_text(size = 12),
        axis.text.x = element_text(angle = 45, size = 6),
        strip.background = element_rect(fill = "black"),
        strip.text = element_text(color = "white", face = "bold"),
        legend.position = "none") 
```

The mean (black point in the plot) of average proportion is ordered as Very good > Good > Excellent > Fair > Poor from the highest and the lowest. While the mean of average proportion of responses has some gaps between response categories over years, it is similar over time within the category. The poor category seems to have small deviations.        


# Problem 2

First, I loaded the dataset for problem 2 from the following library.

```{r instacart_import}
library(p8105.datasets)
data("instacart") 
```

This `instacart` data is composed of **`r nrow(instacart)`** rows and **`r ncol(instacart)`** columns, where each row is corresponding to a product from an order. Each column represents as follows:

* `order_id`: order identifier
* `product_id`: product identifier
* `add_to_cart_order`: order in which each product was added to cart
* `reordered`: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
* `user_id`: customer identifier
* `eval_set`: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval_set)
* `order_number`: the order sequence number for this user (1=first, n=nth)
* `order_dow`: the day of the week on which the order was placed
* `order_hour_of_day`: the hour of the day on which the order was placed
* `days_since_prior_order`: days since the last order, capped at 30, NA if order_number=1
* `product_name`: name of the product
* `aisle_id`: aisle identifier
* `department_id`: department identifier
* `aisle`: the name of the aisle
* `department`: the name of the department

From key variables such as `reordered`, `user_id`, `order_dow`, `order_hour_of_day`, `product_name`, `aisle` and `department`, some interesting findings are as below:

* Fresh fruits and vegetables are reordered frequently. 

```{r reorder}
instacart %>% 
  group_by(aisle) %>% 
  filter(reordered == 1) %>%
  count() %>% 
  arrange(desc(n))
```

* Orders tend to be made from 10 am to 5 pm. 

```{r order_hour}
instacart %>% 
  count(order_hour_of_day) %>% 
  arrange(desc(n))
```

* In general, 9 purchases were made per one person and 80 purchases at max.  

```{r purchase_per_person}
instacart %>% 
  count(user_id) %>% 
  summarize(order_max = max(n), 
            order_min = min(n),
            order_median = median(n), 
            order_mean = round(mean(n), 1)) %>% 
  knitr::kable()
```

* There are **`r instacart %>% distinct(aisle) %>% count()`** aisles in the dataset. 

* The aisles below are the most items ordered from. 

```{r popular_item}
instacart %>% 
  count(aisle) %>% 
  top_n(3) %>% 
  knitr::kable()
```

A plot below shows the number of items ordered in each aisle. 

```{r order_aisle, fig.height = 20}
instacart %>%
  count(aisle) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n, .asc = TRUE)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_bar(stat = "identity",                 ## bar represents values not count
           width = .3,
           fill = "#E69F00") +
  labs(title = "Number of items ordered in each aisle",
       x = "Aisle",
       y = "Number of orders") +
  theme(plot.title = element_text(size = 12),
        axis.text.y = element_text(hjust = 1),
        axis.text = element_text(size = 7.5)) +
  coord_flip()
```

As shown above, fresh vegetables, fresh fruits and packaged vegetables fruits are the top 3 popular items. Products such as beauty, frozen juice and baby accessories are the least purchased items. 

Following table shows the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r item_b_d_p}
instacart %>%
  filter(aisle %in% c("baking ingredients", 
                      "dog food care", 
                      "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  filter(min_rank(desc(n)) == 1) %>% 
  rename("Aisle" = aisle,
         "Product name" = product_name,
         "Number of orders" = n) %>% 
  knitr::kable()
```

Light brown sugar, snack sticks chicken & rice recipe dog treats and organic baby spinach are the most purchased item in baking ingredients, dog food care and packaged vegetables fruits aisle, respectively. Even though these products are the most popular item in each aisle, the number of orders are different from each other. 

A table below shows the mean hour of the day with the scale of 24-hour clock at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. For more readable table, I converted numeric `order_dow` variable to characteristic `days_of_the_week` variable in a way that from `0` to `6` to from `Sunday` to `Saturday`.

```{r apple_icecream}
dow_names = 
  c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")

instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_order_hod = round(mean(order_hour_of_day), 1)) %>% 
  mutate(order_dow = factor(dow_names[order_dow + 1], levels = dow_names)) %>% 
  spread(key = order_dow, value = mean_order_hod) %>% 
  rename("Product name" = product_name) %>% 
  knitr::kable()
```

On Friday, coffe ice cream is ordered earlier than other days of the week and on Tuesday the other way around. Pink lady apples are purchased around lunch time between 11 am to 2 pm.   


# Problem 3

```{r ny_noaa_import}
library(p8105.datasets)
data("ny_noaa")
```

This `ny_noaa` data has **`r nrow(ny_noaa)`** rows and **`r ncol(ny_noaa)`** columns, containing following variables:

* `id`: Weather station ID
* `date`: Date of observation
* `prcp`: Precipitation (tenths of mm)
* `snow`: Snowfall (mm)
* `snwd`: Snow depth (mm)
* `tmax`: Maximum temperature (tenths of degrees C)
* `tmin`: Minimum temperature (tenths of degrees C)

First, I did some data cleaning by creating separate variables for year, month, and day and converted their variable types. Also, I entered omitted decimal points on `prcp`, `tmax` and `tmin` variables.

```{r ny_roaa_clean}
ny_noaa =
  ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(year = as.integer(year),
         month = factor(month.name[as.integer(month)], levels = month.name),
         day = as.integer(day),
         prcp = prcp / 10,
         tmax = as.numeric(tmax) / 10,
         tmin = as.numeric(tmin) / 10)
```

In this dataset, `prcp`, `snow`, `snwd`, `tmax` and `tmin` can be key variables. However, this dataset has a lot of missing data so I calculated the proportion of missing data per each key variables. 

```{r missing}
ny_noaa %>%
  summarise_at(vars(prcp:tmin), .funs = function(x) mean(is.na(x))) %>%
  knitr::kable(digits = 2)
```

It is notable that a large proportion of data was not observed up to 44% per one column. I assumed that if data for months in summer/winter is missing, it will highly affect the result of precipitation and snowfall analysis. Therefore, I explored the proportion of missing data per month. 

```{r missing_per_mo}
ny_noaa %>%
  group_by(month) %>%
  summarise_at(vars(prcp:tmin), .funs = function(x) mean(is.na(x))) %>%
  knitr::kable(digits = 2)
```

As shown above, missing data are well distributed in `prcp`, `tmax` and `tmin` variables over month but `snow` and `snwd` variables have more missing data in winter season in NY (November to February) which, in turn, means that the interpretation for snow data, especially, may not exactly correct. 

The table below indicates 0 mm as the most frequently observed value of snowfall, implying that in most of days there are no snows in NY.

```{r snowfall_count}
ny_noaa %>%
  count(snow) %>%
  arrange(desc(n)) %>%
  top_n(3) %>%
  rename("Snowfall(mm)" = snow,
         "Number of observations" = n) %>%
  knitr::kable()
```

Next, I made a two-panel plot showing the average max temperature in January and in July in each station across years. 

```{r Jan_Jul_av_tmax}
ny_noaa %>%
  filter(month %in% c("January", "July")) %>%
  group_by(id, year, month) %>%
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_tmax, group = id)) +
    geom_line(aes(color = id)) +
    labs(
      title = "Average max temperature in each station across years",
      x = "Year",
      y = "Maximum temperature(°C)"
    ) +
    facet_grid(~ month) +
  viridis::scale_color_viridis(
    name = "id",
    discrete = TRUE
    ) + 
  theme(plot.title = element_text(size = 12),
        axis.text.x = element_text(angle = 45, size = 10),
        strip.background = element_rect(fill = "black"),
        strip.text = element_text(color = "white", face = "bold"),
        legend.position = "none") 
```

As expected, maximum temperature is around 0°C in January and 27°C in July. Compared to 1980, the maximum temperature is getting higher, recently. This spaghetti plot makes it easy to read the trend of maximum temperature but since there are lots of overlapped lines which corresponding to one weather station ID, it is hard to distinguish station IDs. Some outliers are shown in both January and July. 

The final plot is a two-panel plot showing (i) tmax vs tmin for the full dataset and (ii) distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r two_panel, fig.width = 6, fig.asp = .6, out.width = "90%"}

library(patchwork)

## (i) tmax vs tmin for the full dataset
library(hexbin)

tmax_tmin =
  ny_noaa %>%
  filter(!is.na(tmax),
         !is.na(tmin)) %>%
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  labs(title = "Temperature from 1981-2010",
       x = "Maximum temperature(°C)",
       y = "Minimum temperature(°C)") +
  viridis::scale_fill_viridis(
    name = "Count") +
  theme(plot.title = element_text(size = 9),
        axis.title = element_text(size = 7),
        legend.position = "bottom",
        legend.key.width = unit(0.5, "in"),
        legend.text = element_text(size = 5)) 


## (ii) distribution of snowfall values greater than 0 and less than 100 separately by year
d_snow =
  ny_noaa %>%
  filter(snow > 0, snow < 100) %>%
  ggplot(aes(x = snow, color = factor(year))) +
  geom_density() +
  labs(
    x = "Snowfall(mm)",
    y = "Density",
    title = "Distribution of snowfall, 0 ~ 100(mm)"
  ) +
  viridis::scale_color_viridis(
    name = "Year",
    discrete = TRUE) +
  theme(plot.title = element_text(size = 9),
        legend.position = "none",
        axis.title = element_text(size = 7),
        legend.text = element_text(size = 5)) 

## Patch (i) and (ii)
tmax_tmin + d_snow 
```

Left plot of this two-panel figure shows the joint distribution of maximum and minimum temperature. It tells us that most days have maximum 25°C and minimum 15°C temperature during 1981 - 2010. Also, the general temperature range in NY seems to be -15°C/0°C to 20°C/30°C.
Right plot indicates that normal amount of snowfall is 0 ~ 25mm and sometimes NY have 50 and 75mm snowfall.  

